import streamlit as st
import pandas as pd
import warnings
import openpyxl
from webdriver_manager.chrome import ChromeDriverManager

warnings.filterwarnings('ignore')

import requests
import json

from sklearn.metrics.pairwise import cosine_similarity

from selenium import webdriver
from selenium.webdriver.common.by import By

import time

from googletrans import Translator

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')

from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

data = pd.read_excel('final_data.xlsx',index_col=0)

st.subheader('ğŸ“— ì¶”ì²œ ë„ì„œ ë¦¬ìŠ¤íŠ¸')
with st.expander('AF : ê°€ì‚¬ : í‚¤ì›Œë“œ = 0.8 : 0.1 : 0.1 ì¸ ê²½ìš°'):
    col1,col2,col3= st.columns([1,1,1])
    with col1:
        st.markdown('**1. ì°¸ì„ ìˆ˜ ì—†ëŠ” ì¡´ì¬ì˜ ê°€ë²¼ì›€**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/71f6da54-1ac8-4581-8f77-f55ed5c56dbc/image.png')
        st.caption('ì‚¬ë‘ì€ ì€ìœ ë¡œ ì‹œì‘ëœë‹¤. ë‹¬ë¦¬ ë§í•˜ìë©´, í•œ ì—¬ìê°€ ì–¸ì–´ë¥¼ í†µí•´ ìš°ë¦¬ì˜ ì‹œì  ê¸°ì–µì— ì•„ë¡œìƒˆê²¨ì§€ëŠ” ìˆœê°„, ì‚¬ë‘ì€ ì‹œì‘ë˜ëŠ” ê²ƒì´ë‹¤.')
        st.caption('ê·¸ë“¤ì€ ì„œë¡œ ì‚¬ë‘í–ˆëŠ”ë°ë„ ìƒëŒ€ë°©ì—ê²Œ í•˜ë‚˜ì˜ ì§€ì˜¥ì„ ì„ ì‚¬í–ˆë‹¤.')
    with col2:
        st.markdown('**2. ì´ì„±ê³¼ ê°ì„±**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/74f8850c-4d18-42f5-ac29-7e3a1f4f68f4/image.png')
        st.caption('"ì´ì„±"ê³¼ "ê°ì„±"ì´ë¼ëŠ” ë‘ ê°€ì§€ ì¸ê°„ì„±ì„ ì—°ì• ì™€ ê²°í˜¼ì´ë¼ëŠ” ë³´í¸ì  ì£¼ì œë¥¼ í†µí•œ ê³ ì°°')
    with col3:
        st.markdown('**3. ì§€ê¸ˆ, ë§Œë‚˜ëŸ¬ ê°‘ë‹ˆë‹¤**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/fca0c4eb-51bb-4b3c-93d0-866ed37f60fc/image.png')
        st.caption('ë‹¹ì‹ ì—ê² ìˆë‚˜ìš”? ê¸°ì ê°™ì€ ë‹¨ í•œì‚¬ëŒ')
        st.caption('ê·¸ ì‚¬ëŒì„ ë‹¤ì‹œ í•œ ë²ˆ ë§Œë‚  ìˆ˜ ìˆë‹¤ë©´.')
        st.caption('')
        st.caption('ë” ì´ìƒ ë³¼ ìˆ˜ ì—†ê²Œ ëœ ê·¸ë¦¬ìš´ ì‚¬ëŒê³¼ì˜ ê¸°ì  ê°™ì€ ì¬íšŒë¥¼ ê·¸ë¦°ë‹¤. 1ë…„ ì „ ì„¸ìƒì„ ë– ë‚œ ì•„ë‚´ ë¯¸ì˜¤ë¥¼ ê·¸ë¦¬ì›Œí•˜ë©° í•˜ë£¨í•˜ë£¨ë¥¼ ë³´ë‚´ëŠ” ë‹¤ì¿ ë¯¸ëŠ” ë¹„ ì˜¤ëŠ” ë‚  ì•„ë“¤ ìœ ì§€ì™€ í•¨ê»˜ ì°¾ì€ ìˆ²ì†ì—ì„œ ë†€ëê²Œë„ ì£½ì€ ë¯¸ì˜¤ì™€ ì¬íšŒí•œë‹¤. ì´ì•¼ê¸°ëŠ” ëˆ„êµ¬ë³´ë‹¤ ì°¨ê·¼ì°¨ê·¼ ë§ˆìŒì„ ìŒ“ì•„ê°€ë©° ëŠë¦¬ê²Œ ì‚¬ë‘í•´ì˜¨ ë‘ ì‚¬ëŒì˜ ê³¼ê±°ë¡œ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°„ë‹¤.')
    
    col4,col5,col6= st.columns([1,1,1])
    with col4:
        st.markdown('**4. ëª¨ìˆœ**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/66e26a96-c2d5-4c15-a931-66093fb0798e/image.png')
        st.caption('ì¸ìƒì€ íƒêµ¬í•˜ë©´ì„œ ì‚´ì•„ê°€ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‚´ì•„ê°€ë©´ì„œ íƒêµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì‹¤ìˆ˜ëŠ” ë˜í’€ì´ëœë‹¤. ê·¸ê²ƒì´ ì¸ìƒì´ë‹¤â€¦â€¦.')
        st.caption('ë°”ë¡œ ê·¸ ì´ìœ  ë•Œë¬¸ì— ì‚¬ë‘ì„ ì‹œì‘í–ˆê³ , ë°”ë¡œ ê·¸ ì´ìœ  ë•Œë¬¸ì— ë¯¸ì›Œí•˜ê²Œ ëœë‹¤ëŠ”, ì¸ê°„ì´ë€ ì¡´ì¬ì˜ í•œì—†ëŠ” ëª¨ìˆœ......')
    with col5:
        st.markdown('**5. ì‚¬ë‘ì˜ íŒŒê´´**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/0192b5f2-7218-42b7-a5dc-7fa66f654f1c/image.png')
        st.caption('ì—˜ë ˆë‚˜ëŠ” ìì‹ ì„ ìœ„í•´ì„œ ë‚´ê°€ ë‚˜ ìì‹ ì„ íŒŒê´´í•˜ê¸°ë¥¼ ì›í•˜ê³  ìˆì—ˆë‹¤.')
        st.caption('ì‚¬ë‘í•˜ëŠ” ë§Œí¼ ì‚¬ë‘ë°›ê³ ì í•˜ëŠ” ìš•ë§, ìˆœì§„í•˜ê¸°ì— ë”ìš±ë” ì”í˜¹í•œ ìœ ë…„ì˜ ì‚¬ë‘')
    with col6:
        st.markdown('**6. ì œì¸ì—ì–´**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/e2c36e8d-7446-44af-a387-3f43e968d713/image.png')
        st.caption('ìˆœì‘í•˜ê³  ì¸ë‚´í•˜ë©° ë´‰ì‚¬í•˜ëŠ” ì—¬ì„±ì´ ì´ìƒì ìœ¼ë¡œ ì—¬ê²¨ì§€ë˜ ë¹…í† ë¦¬ì•„ ì‹œëŒ€ì—, í˜„ì‹¤ì ì¸ ì¡°ê±´ì´ë‚˜ ê°œì¸ì  ìì§ˆì—ì„œ ì´ì™€ ë™ë–¨ì–´ì§„ ì—¬ì„±ì¸ ì œì¸ì˜ ì„±ì¥ì„ í†µí•´ ë‹¹ëŒ€ ì—¬ì„±ì˜ ì‚¶ ì „ë°˜, ì¦‰ ì—¬ì„±ì˜ êµìœ¡, ê³ ìš©, ì‚¬ë‘, ê²°í˜¼ì— ëŒ€í•œ ì˜ë¬¸')
        
    col7,col8,col9= st.columns([1,1,1])
    with col7:
        st.markdown('**7. ë¬´ì˜ë¯¸ì˜ ì¶•ì œ**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/8e967400-dc99-4d63-8ed1-94ab93396b0e/image.png')
        st.caption('ë³´ì˜ê²ƒì—†ëŠ” ê²ƒì„ ì‚¬ë‘í•´ì•¼ í•´ìš”,ì‚¬ë‘í•˜ëŠ” ë²•ì„ ë°°ì›Œì•¼ í•´ìš”.')
        st.caption('ë†ë‹´ê³¼ ê±°ì§“ë§, ì˜ë¯¸ì™€ ë¬´ì˜ë¯¸, ì¼ìƒê³¼ ì¶•ì œì˜ ê²½ê³„ì—ì„œì‚¶ê³¼ ì¸ê°„ì˜ ë³¸ì§ˆì„ ë°”ë¼ë³´ëŠ” ì‹œì„ ')
    with col8:
        st.markdown('**8. 80ì¼ê°„ì˜ ì„¸ê³„ì¼ì£¼**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/8875c9d4-d568-4242-a478-4358e97411df/image.png')
        st.caption('2ë§Œ íŒŒìš´ë“œë¥¼ ê±¸ê³  80ì¼ ë™ì•ˆì˜ ì„¸ê³„ ì¼ì£¼ì— ë‚˜ì„  ì˜êµ­ ì‹ ì‚¬ í•„ë¦¬ì–´ìŠ¤ í¬ê·¸. ')
        st.caption('ê·¸ëŠ” ê¸°ê³„ì²˜ëŸ¼ ì •í™•í•˜ê³  ëƒ‰ì •í•œ ì˜êµ­ ì‹ ì‚¬ë‹¤. í•œ ì¹˜ì˜ ì˜¤ì°¨ë„ ì—†ì´ ì—¬í–‰ì„ ê³„íší•˜ëŠ” ì£¼ì¸ê³µì„ í†µí•´ ì¥˜ ë² ë¥¸ì€ ì¹˜ë°€í•˜ê³  ê³¼í•™ì ì´ë©° ì´ì„±ì ì¸ ì¸ê°„ê³¼, ì¸ê°„ì— ëŒ€í•œ ì‹ ë¢°ì™€ ì• ì • ê·¸ë¦¬ê³  ì„¸ê³„ì— ëŒ€í•œ ê¸ì •ìœ¼ë¡œ ì°¨ ìˆëŠ” ì¸ê°„ìƒì„ ê·¸ë ¤ ë‚¸ë‹¤.')
    with col9:
        st.markdown('**9. ëª¬í…Œí¬ë¦¬ìŠ¤í†  ë°±ì‘**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/aef3eb6b-e3d9-4745-8b7b-07e592e4637b/image.png')
        st.caption('ëª¨ë“  ì•…ì—ëŠ” ë‘ ê°œì˜ ì•½ì´ ìˆë‹¤. ì‹œê°„ê³¼ ì¹¨ë¬µì´ ê·¸ê²ƒì´ë‹¤')
        st.caption('ì¸ê°„ì‚¬ì—ì„œ ê°€ì¥ í¥ê²¨ìš´ ì´ì•¼ê¸°ëŠ” ë¶ˆí–‰ì„ ë”›ê³  í–‰ë³µì„ ë˜ì°¾ëŠ” ì´ì•¼ê¸°ê°€ ì•„ë‹ê¹Œ?')
        st.caption('ëª¨ëµê³¼ í•¨ì •ì— ë¹ ì§€ì§€ë§Œ, ë¶€ì™€ ëª…ì˜ˆë¥¼ íšŒë³µí•˜ì—¬ í™”ë ¤í•˜ê²Œ ë³µìˆ˜í•œë‹¤ëŠ” ì´ì•¼ê¸°ì— ì‚¬ëŒë“¤ì€ ì‰½ê²Œ ì—´ê´‘í•œë‹¤.')
        st.caption('<ëª¬í…Œí¬ë¦¬ìŠ¤í†  ë°±ì‘>ì´ ëŒ€í‘œì ì¸ ê²½ìš°. ë°°ì‹ , ì–µìš¸í•œ ê°ê¸ˆ, ë³µìˆ˜ ì´ 3ìš”ì†ŒëŠ” ì‹œëŒ€ë¥¼ ë¶ˆë¬¸í•˜ê³  ë…ìë“¤ì„ ë§¤ë£Œì‹œì¼°ë‹¤.')

    col10,col11,col12 = st.columns([1,1,1])
    with col10:
        st.markdown('**10. í˜ë“œë¥´ì™€ ì´í´ë¦¬íŠ¸**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/fc8c56f5-5661-427b-bb39-70c6e6169fe4/image.png')
        st.caption('ì¸ê°„ì€ ì§„ì • ìì‹ ì„ ì˜¥ì£„ëŠ” ì •ë…ìœ¼ë¡œë¶€í„° ìŠ¤ìŠ¤ë¡œë¥¼ êµ¬í•  ì˜ì§€ë„, ëŠ¥ë ¥ë„ ì—†ëŠ” ì¡´ì¬ì¸ê°€.')
        st.caption('ì—ìš°ë¦¬í”¼ë°ìŠ¤ì˜ ã€Œíˆí´ë¦¬í† ìŠ¤ã€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë…ì´ ì§€ë‹Œ íŒŒê´´ì  ë³¸ì„±,í†µì œí•  ìˆ˜ ì—†ëŠ” ì •ë…ì— ë¹ ì§„ í•œ ì¸ê°„ì´ ë³´ì—¬ ì£¼ëŠ” ê°ì •ì˜ ê²©ì •ì„ íŒŒê³ ë“  ë¼ì‹  ë¹„ê·¹ì˜ ì •ìˆ˜.')
    with col11:
        st.markdown('**11. ê²°í˜¼ã†ì—¬ë¦„**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/81bbcf18-81e5-4952-853b-927bb8c2223d/image.png')
        st.caption('ê¹Šì´ ì‚¬ë‘í•˜ëŠ” ì—¬ì¸ì˜ ë§¤ë ¥ì„ í•­ëª©ë³„ë¡œ ì¡°ëª©ì¡°ëª© ìŠì„ ìˆ˜ ìˆê² ëŠ”ê°€?ê·¸ëŸ´ ìˆ˜ ì—†ë‹¤, ê·¸ëƒ¥ ì „ì²´ë¥¼ ì‚¬ë‘í•˜ëŠ” ê²ƒì´ë‹¤.')
        st.caption('ì¹´ë®ˆ ì‚¬ìƒì˜ í•µì‹¬ì¸ â€˜ë¶€ì¡°ë¦¬â€™ì™€ â€˜ë°˜í•­â€™ì˜ ì¶œë°œ ë° ì™„ì„± ê³¼ì •ì´ ìœ¡ì„±ìœ¼ë¡œ ë“¤ë¦¬ëŠ” ë“¯í•œ ìì „ì  ê¸°ë¡')

with st.expander('ê°€ì‚¬ ì¤‘ì‹¬ì¸ ê²½ìš°'):
    col1,col2,col3= st.columns([1,1,1])
    with col1:
        st.markdown('**1. ì°¸ì„ ìˆ˜ ì—†ëŠ” ì¡´ì¬ì˜ ê°€ë²¼ì›€**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/71f6da54-1ac8-4581-8f77-f55ed5c56dbc/image.png')
        st.caption('ì‚¬ë‘ì€ ì€ìœ ë¡œ ì‹œì‘ëœë‹¤. ë‹¬ë¦¬ ë§í•˜ìë©´, í•œ ì—¬ìê°€ ì–¸ì–´ë¥¼ í†µí•´ ìš°ë¦¬ì˜ ì‹œì  ê¸°ì–µì— ì•„ë¡œìƒˆê²¨ì§€ëŠ” ìˆœê°„, ì‚¬ë‘ì€ ì‹œì‘ë˜ëŠ” ê²ƒì´ë‹¤.')
        st.caption('ê·¸ë“¤ì€ ì„œë¡œ ì‚¬ë‘í–ˆëŠ”ë°ë„ ìƒëŒ€ë°©ì—ê²Œ í•˜ë‚˜ì˜ ì§€ì˜¥ì„ ì„ ì‚¬í–ˆë‹¤.')
    with col2:
        st.markdown('**2. ì–´ë¦°ì™•ì**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/93a93b74-d728-4727-81b5-7af32114a2aa/image.png')
        st.caption('ë„¤ê°€ ì˜¤í›„ 4ì‹œì— ì˜¨ë‹¤ë©´ ë‚œ 3ì‹œë¶€í„° ì„¤ë  ê±°ì•¼. 4ì‹œê°€ ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ì ì  ë” í–‰ë³µí•´ì§€ê² ì§€. 4ì‹œê°€ ë˜ë©´ ë‚œ ê°€ìŠ´ì´ ë‘ê·¼ê±°ë ¤ì„œ ì•ˆì ˆë¶€ì ˆëª»í•˜ê³  ê±±ì •ì„ í•  ê±°ì•¼. í–‰ë³µì˜ ëŒ€ê°€ë¥¼ ì•Œê²Œ ë˜ê² ì§€! í•˜ì§€ë§Œ ë„¤ê°€ ì•„ë¬´ ë•Œë‚˜ ì˜¨ë‹¤ë©´ ì–¸ì œë¶€í„° ë§ˆìŒì˜ ì¤€ë¹„ë¥¼ í•´ì•¼ í• ì§€ ë„ë¬´ì§€ ì•Œ ìˆ˜ ì—†ì–ì•„.')
        st.caption('ìˆœìˆ˜ì„±ì„ í—ˆë½í•˜ì§€ ì•ŠëŠ” ì„¸ìƒì—ì„œ ëŠì„ì—†ì´ ë°©í™©í•˜ê³  ê³ ë‡Œí•œ ìƒí…ì¥í˜ë¦¬. ê·¸ëŠ” ì„¸ìƒì„ ë°”ê¿€ ìˆ˜ëŠ” ì—†ì§€ë§Œ í¬ë§ì„ ê·¸ë¦¬ê³  ì‹¶ì—ˆê³ , ìì‹ ì´ ë™ê²½í•˜ê³  í¬ë§í•˜ëŠ” ì‚¶ì„ â€˜ì–´ë¦° ì™•ìâ€™ë¡œ í˜•ìƒí™”í–ˆë‹¤.')
    with col3:
        st.markdown('**3. ëª¬í…Œí¬ë¦¬ìŠ¤í†  ë°±ì‘**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/aef3eb6b-e3d9-4745-8b7b-07e592e4637b/image.png')
        st.caption('ëª¨ë“  ì•…ì—ëŠ” ë‘ ê°œì˜ ì•½ì´ ìˆë‹¤. ì‹œê°„ê³¼ ì¹¨ë¬µì´ ê·¸ê²ƒì´ë‹¤')
        st.caption('ì¸ê°„ì‚¬ì—ì„œ ê°€ì¥ í¥ê²¨ìš´ ì´ì•¼ê¸°ëŠ” ë¶ˆí–‰ì„ ë”›ê³  í–‰ë³µì„ ë˜ì°¾ëŠ” ì´ì•¼ê¸°ê°€ ì•„ë‹ê¹Œ?')
        st.caption('ëª¨ëµê³¼ í•¨ì •ì— ë¹ ì§€ì§€ë§Œ, ë¶€ì™€ ëª…ì˜ˆë¥¼ íšŒë³µí•˜ì—¬ í™”ë ¤í•˜ê²Œ ë³µìˆ˜í•œë‹¤ëŠ” ì´ì•¼ê¸°ì— ì‚¬ëŒë“¤ì€ ì‰½ê²Œ ì—´ê´‘í•œë‹¤.')
        st.caption('<ëª¬í…Œí¬ë¦¬ìŠ¤í†  ë°±ì‘>ì´ ëŒ€í‘œì ì¸ ê²½ìš°. ë°°ì‹ , ì–µìš¸í•œ ê°ê¸ˆ, ë³µìˆ˜ ì´ 3ìš”ì†ŒëŠ” ì‹œëŒ€ë¥¼ ë¶ˆë¬¸í•˜ê³  ë…ìë“¤ì„ ë§¤ë£Œì‹œì¼°ë‹¤.')
    
    col4,col5,col6= st.columns([1,1,1])
    with col4:
        st.markdown('**4. ë¡œë¯¸ì˜¤ì™€ ì¤„ë¦¬ì—£**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/6edfa385-5695-4ba8-a14e-45ce4871f5ca/image.png')
        st.caption('ì˜¤, ë‘¥ê·¼ ê¶¤ë„ ì•ˆì—ì„œ í•œ ë‹¬ ë‚´ë‚´ ë³€í•˜ëŠ”ì§€ì¡° ì—†ëŠ” ë‹¬ì—ê²Œ ë§¹ì„¸í•˜ì§„ ë§ˆì„¸ìš”')
        st.caption('ë‹¤ì³ ë³¸ ì  ì—†ëŠ” ìê°€ í‰í„°ë¥¼ ë¹„ì›ƒëŠ” ë²•â€¦')
        st.caption('ë‹¬ë¹› ì•„ë˜ ì£¼ê³ ë°›ì€ ì²« í‚¤ìŠ¤ì™€ ì‚¬ë‘ì˜ ë§¹ì„¸,ì‚´ì•„ ìˆëŠ” ì£½ìŒì„ í†µí•´ ë„ë‹¬í•˜ëŠ” ì£½ìŒì„ ë„˜ì–´ì„œëŠ” ì‚¬ë‘!ì…°ìµìŠ¤í”¼ì–´ê°€ ë¹šì–´ë‚¸ ìˆœìˆ˜í•œ ì—´ì •ì˜ ë¹„ê·¹, ê·¸ ì‚¬ë‘ì˜ ëª¨ìˆœì–´ë²•')
    with col5:
        st.markdown('**5. ì•„ì£¼ í¸ì•ˆí•œ ì£½ìŒ**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/86c06fae-d47c-462f-b8bf-2fbfadcd9f33/image.png')
        st.caption('ì—„ë§ˆëŠ” ìœ ë…„ ì‹œì ˆ ë‚´ë‚´ ê·œë²”ê³¼ ê¸ˆê¸°ë¼ëŠ” ê°‘ì˜·ì„ ë‘ë¥¸ ì±„ ëª¸ê³¼ ë§ˆìŒ, ì •ì‹ ì„ ì–µì••ë‹¹í–ˆë‹¤. ê·¸ë¦¬ê³  ìŠ¤ìŠ¤ë¡œë¥¼ ëˆìœ¼ë¡œ ì˜­ì•„ë§¤ë„ë¡ êµìœ¡ë°›ì•˜ë‹¤. ê·¸ëŸ° ì—„ë§ˆì˜ ë‚´ë©´ì—ëŠ”ë“ì–´ì˜¤ë¥´ëŠ” í”¼ì™€ ë¶ˆê°™ì€ ì •ì—´ì„ ì§€ë‹Œ í•œ ì—¬ì¸ì´ ì‚´ì•„ ìˆ¨ ì‰¬ê³  ìˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ ì—¬ì¸ì€ ë’¤í‹€ë¦¬ê³  í›¼ì†ëœ ëì— ìê¸° ìì‹ ì—ê²Œì¡°ì°¨ ë‚¯ì„  ì¡´ì¬ê°€ ë˜ì–´ ë²„ë¦° ëª¨ìŠµì´ì—ˆë‹¤.')
        st.caption('ì£¼ì²´ì„±ì„ í¬ê¸°í•˜ë©° íƒ€ìë¡œ ì‚´ë„ë¡ ê°•ìš”ë°›ì•„ ì˜¨ í•œ ì¸ê°„ì˜ ìƒì• , ë‚˜ì•„ê°€ ë‹¹ëŒ€ ì—¬ì„± ì „ì²´ì˜ ëª¨ìŠµ. ')
        st.caption('ëƒ‰ëŒ€í•˜ë©° ì™¸ë©´í–ˆë˜ ì„¸ê³„ë¥¼ ìƒˆë¡­ê²Œ ì¸ì‹í•˜ë©° ìê¸° ì •ì²´ì„±ì˜ ì¼ë¶€ë¡œ ë°›ì•„ë“¤ì´ëŠ” ê³¼ì •ì´ë©°, ê·¸ì™€ ë™ì‹œì— ë‚¨ê³¼ ì—¬, ìœ¡ì²´ì™€ ì •ì‹ , ì‚¶ê³¼ ì£½ìŒ ë“± êµ¬ë³„ ì§“ê¸°ë¡œ ê°€ë“í–ˆë˜ ì¸ê°„ ë‚´ë©´ì˜ ê²½ê³„ë¥¼ í—ˆë¬´ëŠ” ì‘í’ˆ.')
    with col6:
        st.markdown('**6. ë¬´ì˜ë¯¸ì˜ ì¶•ì œ**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/8e967400-dc99-4d63-8ed1-94ab93396b0e/image.png')
        st.caption('ë³´ì˜ê²ƒì—†ëŠ” ê²ƒì„ ì‚¬ë‘í•´ì•¼ í•´ìš”,ì‚¬ë‘í•˜ëŠ” ë²•ì„ ë°°ì›Œì•¼ í•´ìš”.')
        st.caption('ë†ë‹´ê³¼ ê±°ì§“ë§, ì˜ë¯¸ì™€ ë¬´ì˜ë¯¸, ì¼ìƒê³¼ ì¶•ì œì˜ ê²½ê³„ì—ì„œì‚¶ê³¼ ì¸ê°„ì˜ ë³¸ì§ˆì„ ë°”ë¼ë³´ëŠ” ì‹œì„ ')
    col7,col8,col9= st.columns([1,1,1])
    with col7:
        st.markdown('**7. ì˜ëª» ê±¸ë ¤ì˜¨ ì „í™”**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/9fa4d386-e2fa-435d-91bc-7841c5ddd96e/image.png')
        st.caption('ê·¸ëŸ° ì‹ìœ¼ë¡œ ì„¸ì›”ì€ í˜ëŸ¬ê°ˆ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì•…ëª½ ê°™ë˜ ë‚´ ì¸ìƒì˜ ì¥ë©´ë“¤ì´ ëˆˆì— ì„ í•  ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ë‚˜ëŠ” ì´ì œ ê·¸ê²ƒë“¤ë¡œ ì¸í•´ ì•„íŒŒí•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤.')
        st.caption('ì£½ìŒ, ì‚¬ë‘, ê·¸ë¦¬ê³  ìƒì‹¤"ì•„ê³ íƒ€ í¬ë¦¬ìŠ¤í† í”„ì˜ ì‘í’ˆ ì¤‘ ê°€ì¥ ë‚¯ì„¤ê³  ë¹„ë°€ìŠ¤ëŸ¬ìš´ ì•…ëª½ê³¼ ìš°í™”" - ë¥´ ëª½ë“œ(Le Monde)')
    with col8:
        st.markdown('**8. íŒŒìš°ìŠ¤íŠ¸**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/9bc7048f-9e21-43cc-8db8-a71bb1bc8dfa/image.png')
        st.caption('ë‚´ê°€ ë„ˆì˜ ë…¸ì˜ˆê°€ ë˜ì–´ ì´ ì„¸ìƒ ëª¨ë“  ì˜í™”ë¥¼ ì²´í—˜í•˜ê²Œ í•´ì£¼ëŠ” ëŒ€ì‹ ï¼Œë„¤ê°€ ì–´ëŠ í•œìˆœê°„ `ë©ˆì¶”ì–´ë¼ï¼ë„ˆëŠ” ë„ˆë¬´ë„ ì•„ë¦„ë‹µë‹¤â€™ë¼ë©° íœ´ì‹ì„ ì›í•˜ë©´ ê·¸ë•Œë¶€í„° ë„ˆì˜ ì˜í˜¼ì€ ì˜ì›íˆ ë‚˜ì˜ ê²ƒì´ë‹¤.')
        st.caption('ì§€ì‹ê³¼ í•™ë¬¸ì— ì ˆë§í•œ ë…¸í•™ì íŒŒìš°ìŠ¤íŠ¸ ë°•ì‚¬ì˜ ë¯¸ë§(è¿·å¦„)ê³¼ êµ¬ì›ì˜ ì¥êµ¬í•œ ë…¸ì •ì„ ê·¸ë¦°ë‹¤. ì•…ë§ˆ ë©”í”¼ìŠ¤í† í ë ˆìŠ¤ì˜ ìœ í˜¹ì— ë¹ ì ¸ í˜„ì„¸ì˜ ì¾Œë½ì„ ì«“ìœ¼ë©° ë°©í™©í•˜ë˜ íŒŒìš°ìŠ¤íŠ¸ëŠ” ë§ˆì¹¨ë‚´ ì˜ëª»ì„ ê¹¨ë‹«ê³  ì²œìƒì˜ êµ¬ì›ì„ ë°›ëŠ”ë‹¤.')
    with col9:
        st.markdown('**9. ì–´ë–»ê²Œë“  ì´ë³„**')
        st.image('https://velog.velcdn.com/images/jeo0534/post/10ca695a-924b-44be-a573-1227e1525510/image.png')
        st.caption('ì´ ê³„ì ˆì€ ì¡°ê¸ˆ ê°€ë²¼ìš´ ì ˆë§ì„ ì•“ê¸°ì— ì–¼ë§ˆë‚˜ ì°¬ë€í•œê°€')
        st.caption('ì‚¬ë‘, ê²°êµ­ì—ëŠ” ì´ë³„, ëë‚´ ë¶ˆê°€í”¼í•œ ê³ ë…ì§€ê·¹í•œ ìƒì²˜ ì•ˆì— ì›ƒìŒì„ í’ˆì€ ì“¸ì“¸í•œ í†µì°°')

st.divider()

############################
st.header('1ï¸âƒ£ ì±… ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.')

book_title =  st.text_input(label = 'ì˜ˆì‹œ) ë‚ ì”¨ê°€ ì¢‹ìœ¼ë©´ ì°¾ì•„ê°€ê² ì–´ìš”',value="",key='text')

def reset():
    st.session_state.text = ""

reset = st.button('Reset',on_click=reset)
if not book_title:
        con = st.container()
        con.caption('Result')
        con.error('ì±… ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.',icon="âš ï¸")
        st.stop()


rest_api_key = "41d651c93152d5ec054dc828cacfa671"
url = "https://dapi.kakao.com/v3/search/book"
header = {"authorization": "KakaoAK "+rest_api_key}
querynum = {"query": book_title}

try:
    response = requests.get(url, headers=header, params = querynum)
    content = response.text
    ì±…ì •ë³´ = json.loads(content)['documents'][0]
except:
    con = st.container()
    con.caption('Result')
    con.error('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì±…ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.',icon="ğŸš¨")
    st.stop()

book = pd.DataFrame({'title': ì±…ì •ë³´['title'],
              'isbn': ì±…ì •ë³´['isbn'],
              'authors': ì±…ì •ë³´['authors'],
              'publisher': ì±…ì •ë³´['publisher']})

target_url = ì±…ì •ë³´['url']


# ì˜µì…˜ ìƒì„±
options = webdriver.ChromeOptions()
# ì°½ ìˆ¨ê¸°ëŠ” ì˜µì…˜ ì¶”ê°€
options.add_argument("headless")

#driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=options)
driver = webdriver.Chrome('/Users/seoeunseo/Desktop/deep.daiv/á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/project_run/chromedriver.exe', options=options)
driver.get(target_url)

img= driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[1]/div[1]/span/img')
img_src = img.get_attribute('src')

col1, col2 = st.columns([1,2])
with col1:
    st.image(img_src,width=150)
with col2:
    title = book['title'][0]
    author = book['authors'][0]
    publisher = book['publisher'][0]
    
    st.caption('ì œëª© : '+ title)
    st.caption('ì €ì : '+ author)
    st.caption('ì¶œì‚°ì‚¬ : '+publisher)

st.title('')
text = '<'+title +'>ì— ëŒ€í•œ ì •ë³´ë¥¼ ëª¨ìœ¼ê³  ìˆëŠ” ì¤‘ì…ë‹ˆë‹¤.'
my_bar = st.progress(0, text=text)
time.sleep(5)
my_bar.progress(5, text='ã€°ï¸5%ã€°ï¸')

try :
    botton = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[2]/div[3]/a')
    botton.click()
except :
    pass
ì±…ì†Œê°œ = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[2]/p')

time.sleep(3)
my_bar.progress(10, text='ã€°ï¸10%ã€°ï¸')
try :
    botton = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[5]/div[3]/a')
    botton.click()
except :
    pass
ì±…ì†ìœ¼ë¡œ = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[5]/p')

time.sleep(3)
my_bar.progress(20, text='ã€°ï¸20%ã€°ï¸')
try :
    botton = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[6]/div[3]/a')
    botton.click()
except :
    pass
ì„œí‰ = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[6]/p')


book['ì±…ì†Œê°œ'] = ì±…ì†Œê°œ.text
book['ì±…ì†ìœ¼ë¡œ'] = ì±…ì†ìœ¼ë¡œ.text
book['ì„œí‰'] = ì„œí‰.text
driver.close()

time.sleep(1)
my_bar.progress(30, text='ã€°ï¸30%ã€°ï¸')


#ì˜ì–´ ë¶ˆìš©ì–´ ì‚¬ì „
stops = set(stopwords.words('english'))

def hapus_url(text):
    mention_pattern = r'@[\w]+'
    cleaned_text = re.sub(mention_pattern, '', text)
    return re.sub(r'http\S+','', cleaned_text)

#íŠ¹ìˆ˜ë¬¸ì ì œê±°
#ì˜ì–´ ëŒ€ì†Œë¬¸ì, ìˆ«ì, ê³µë°±ë¬¸ì(ìŠ¤í˜ì´ìŠ¤, íƒ­, ì¤„ë°”ê¿ˆ ë“±) ì•„ë‹Œ ë¬¸ìë“¤ ì œê±°
def remove_special_characters(text, remove_digits=True):
    text=re.sub(r'[^a-zA-Z0-9\s]', '', text)
    return text


#ë¶ˆìš©ì–´ ì œê±°
def delete_stops(text):
    text = text.lower().split()
    text = ' '.join([word for word in text if word not in stops])
    return text
   
    
#í’ˆì‚¬ tag ë§¤ì¹­ìš© í•¨ìˆ˜
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
    

#í’ˆì‚¬ íƒœê¹… + í‘œì œì–´ ì¶”ì¶œ
def tockenize(text):
    tokens=word_tokenize(text)
    pos_tokens=nltk.pos_tag(tokens)
    
    text_t=list()
    for _ in pos_tokens:
        text_t.append([_[0], get_wordnet_pos(_[1])])
    
    lemmatizer = WordNetLemmatizer()
    text = ' '.join([lemmatizer.lemmatize(word[0], word[1]) for word in text_t])
    return text

def clean(text):
    text = remove_special_characters(text, remove_digits=True)
    text = delete_stops(text)
    text = tockenize(text)
    return text


translator = Translator()
for col in ['ì±…ì†Œê°œ', 'ì±…ì†ìœ¼ë¡œ', 'ì„œí‰']:
    name = col+'_trans'
    if book[col].values == '':
        book[name] = ''
        continue
    book[name] = clean(translator.translate(hapus_url(book.loc[0, col])).text)

total_text = book.loc[0, 'ì±…ì†Œê°œ_trans'] + book.loc[0, 'ì±…ì†ìœ¼ë¡œ_trans'] + book.loc[0, 'ì„œí‰_trans']



df = pd.read_csv('tweet_data_agumentation.csv', index_col = 0)

tfidf_vect_emo = TfidfVectorizer()
tfidf_vect_emo.fit_transform(df["content"])

model = joblib.load('SVM.pkl')
total_text2 = tfidf_vect_emo.transform(pd.Series(total_text))
model.predict_proba(total_text2)
sentiment = pd.DataFrame(model.predict_proba(total_text2),index=['prob']).T
sentiment['ê°ì •'] = ['empty','sadness','enthusiasm','worry','love','fun','hate','happiness','boredom','relief','anger']
sentiment2 = sentiment.sort_values(by='prob',ascending=False)

my_bar.progress(60, text='ã€°ï¸60%ã€°ï¸')

# audio featureë‘ text ê°ì •
audio_data = data.iloc[:,-12:-1]

sentiment_prob = sentiment['prob']
sentiment_prob.index = sentiment['ê°ì •']

audio_data.columns = ['empty', 'sadness', 'enthusiasm', 'worry', 'love', 'fun', 'hate',
       'happiness', 'boredom', 'relief', 'anger']

audio_data_1 = pd.concat([sentiment_prob,audio_data.T],axis=1).T

col = ['book']+list(data['name'])
cosine_sim_audio = cosine_similarity(audio_data_1)
cosine_sim_audio_df = pd.DataFrame(cosine_sim_audio, index = col, columns=col)

audio_sim = cosine_sim_audio_df['book']

# ê°€ì‚¬ë‘ text
lyrics_data = data.iloc[:,5:-12]
lyrics_data_1 = pd.concat([sentiment_prob,lyrics_data.T],axis=1).T
cosine_sim_lyrics = cosine_similarity(lyrics_data_1)
cosine_sim_lyrics_df = pd.DataFrame(cosine_sim_lyrics, index =col, columns=col)
lyrics_sim = cosine_sim_lyrics_df['book']
my_bar.progress(80, text='ã€°ï¸80%ã€°ï¸')

# í‚¤ì›Œë“œë‘ text
keyword_data = data['key_word']
book_song_cont1 = pd.DataFrame({"text": total_text}, index = range(1))
book_song_cont2 = pd.DataFrame({"text": keyword_data})
keyword_data_1 = pd.concat([book_song_cont1, book_song_cont2], axis=0).reset_index(drop=True)

tfidf_vect_cont = TfidfVectorizer()
tfidf_matrix_cont = tfidf_vect_cont.fit_transform(keyword_data_1['text'])
tfidf_array_cont = tfidf_matrix_cont.toarray()
tfidf_df_cont = pd.DataFrame(tfidf_array_cont, columns=tfidf_vect_cont.get_feature_names_out())

cosine_sim_keyword = cosine_similarity(tfidf_array_cont)
cosine_sim_keyword_df = pd.DataFrame(cosine_sim_keyword, index = col, columns=col)
keyword_sim = cosine_sim_keyword_df['book']

# ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚°
total_sim  = 0.8*audio_sim + 0.1*lyrics_sim + 0.1*keyword_sim

recommend_song = total_sim.sort_values(ascending=False)[1:6].index
total_sim_df = pd.DataFrame(total_sim[1:])
total_sim_df = total_sim_df.reset_index()
total_sim_df.columns = ['name','book']

top_five = total_sim_df.sort_values(by='book',ascending=False)[:5]
index = total_sim_df.sort_values(by='book',ascending=False)[:5].index.sort_values()
artist = data.iloc[index][['url','name','Artist']]
top_five_df = pd.merge(artist,top_five,on='name').sort_values(by='book',ascending=False).drop_duplicates()

total_sim  = 0*audio_sim + 0.5*lyrics_sim + 0.5*keyword_sim

recommend_song = total_sim.sort_values(ascending=False)[1:6].index
total_sim_df_1 = pd.DataFrame(total_sim[1:])
total_sim_df_1 = total_sim_df_1.reset_index()
total_sim_df_1.columns = ['name','book']

top_five_1 = total_sim_df_1.sort_values(by='book',ascending=False)[:5]
index_1 = total_sim_df_1.sort_values(by='book',ascending=False)[:5].index.sort_values()

artist = data.iloc[index_1][['url','name','Artist']]
top_five_df_1 = pd.merge(artist,top_five_1,on='name').sort_values(by='book',ascending=False).drop_duplicates()


my_bar.progress(100, text='100%')
time.sleep(1)
my_bar.empty()


st.caption('ì±… ì†Œê°œ ì¤‘....')
long = book.loc[0, 'ì±…ì†Œê°œ'] + book.loc[0, 'ì±…ì†ìœ¼ë¡œ'] + book.loc[0, 'ì„œí‰']
st.markdown(long[:300]+'...')

st.markdown('')

lyrics = pd.read_excel('lyrics.xlsx',index_col=0)
lyrics_list = []
for i in top_five_df['url']:
    lyrics_list.append(lyrics[i== lyrics['url']]['lyrics'].values[0])
for i in top_five_df_1['url']:
    lyrics_list.append(lyrics[i== lyrics['url']]['lyrics'].values[0])

lyrics_eng_list = []
for i in top_five_df['url']:
    lyrics_eng_list.append(lyrics[i== lyrics['url']]['lyrics_english'].values[0])
for i in top_five_df_1['url']:
    lyrics_eng_list.append(lyrics[i== lyrics['url']]['lyrics_english'].values[0])

st.header('2ï¸âƒ£ ê²°ê³¼')
st.subheader('ğŸ™‚ ë…¸ë˜ì™€ ë¶„ìœ„ê¸°ê°€ ìœ ì‚¬í•œ ë…¸ë˜')
st.caption('AF : ê°€ì‚¬ : í‚¤ì›Œë“œ = 0.8 : 0.1 : 0.1')
tab1, tab2, tab3, tab4, tab5= st.tabs(['TOP 1' , 'TOP 2', 'TOP 3', 'TOP 4', 'TOP 5'])
with tab1:
    st.subheader('ğŸ¥‡ TOP 1')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df.iloc[0]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df.iloc[0]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df.iloc[0]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df.iloc[0]['book']))
    with st.expander('ê°€ì‚¬'):
        st.caption('ì›ë³¸ ver')
        st.markdown(lyrics_list[0])
        st.caption('ì˜ì–´ ver')
        st.markdown(lyrics_eng_list[0])
    st.markdown('')
with tab2:
    st.subheader('ğŸ¥ˆ TOP 2')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df.iloc[1]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df.iloc[1]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df.iloc[1]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df.iloc[1]['book']))
    with st.expander('ê°€ì‚¬'):
        st.caption('ì›ë³¸ ver')
        st.markdown(lyrics_list[1])
        st.caption('ì˜ì–´ ver')
        st.markdown(lyrics_eng_list[1])
    st.markdown('')
with tab3:
    st.subheader('ğŸ¥‰ TOP 3')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df.iloc[2]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df.iloc[2]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df.iloc[2]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df.iloc[2]['book']))
    with st.expander('ê°€ì‚¬'):
        st.caption('ì›ë³¸ ver')
        st.markdown(lyrics_list[2])
        st.caption('ì˜ì–´ ver')
        st.markdown(lyrics_eng_list[2])
    st.markdown('')
with tab4:
    st.subheader('TOP 4')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df.iloc[3]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df.iloc[3]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df.iloc[3]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df.iloc[3]['book']))
    with st.expander('ê°€ì‚¬'):
        st.caption('ì›ë³¸ ver')
        st.markdown(lyrics_list[3])
        st.caption('ì˜ì–´ ver')
        st.markdown(lyrics_eng_list[3])
    st.markdown('')
with tab5:
    st.subheader('TOP 5')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df.iloc[4]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df.iloc[4]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df.iloc[4]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df.iloc[4]['book']))
    with st.expander('ê°€ì‚¬'):
        st.caption('ì›ë³¸ ver')
        st.markdown(lyrics_list[4])
        st.caption('ì˜ì–´ ver')
        st.markdown(lyrics_eng_list[4])

st.subheader('ğŸ“– ë…¸ë˜ì™€ ë‚´ìš©ì´ ìœ ì‚¬í•œ ë…¸ë˜')
st.caption('AF : ê°€ì‚¬ : í‚¤ì›Œë“œ = 0 : 0.5 : 0.5')
tab1, tab2, tab3, tab4, tab5= st.tabs(['TOP 1' , 'TOP 2', 'TOP 3', 'TOP 4', 'TOP 5'])
with tab1:
    st.subheader('ğŸ¥‡ TOP 1')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df_1.iloc[0]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df_1.iloc[0]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df_1.iloc[0]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df_1.iloc[0]['book']))
    with st.expander('ê°€ì‚¬'):
        st.caption('ì›ë³¸ ver')
        st.markdown(lyrics_list[5])
        st.caption('ì˜ì–´ ver')
        st.markdown(lyrics_eng_list[5])
    st.markdown('')
with tab2:
    st.subheader('ğŸ¥ˆ TOP 2')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df_1.iloc[1]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df_1.iloc[1]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df_1.iloc[1]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df_1.iloc[1]['book']))
    with st.expander('ê°€ì‚¬'):
        st.caption('ì›ë³¸ ver')
        st.markdown(lyrics_list[6])
        st.caption('ì˜ì–´ ver')
        st.markdown(lyrics_eng_list[6])
    st.markdown('')
with tab3:
    st.subheader('ğŸ¥‰ TOP 3')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df_1.iloc[2]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df_1.iloc[2]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df_1.iloc[2]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df_1.iloc[2]['book']))
    with st.expander('ê°€ì‚¬'):
        st.caption('ì›ë³¸ ver')
        st.markdown(lyrics_list[7])
        st.caption('ì˜ì–´ ver')
        st.markdown(lyrics_eng_list[7])
    st.markdown('')
with tab4:
    st.subheader('TOP 4')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df_1.iloc[3]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df_1.iloc[3]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df_1.iloc[3]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df_1.iloc[3]['book']))
    with st.expander('ê°€ì‚¬'):
        st.caption('ì›ë³¸ ver')
        st.markdown(lyrics_list[8])
        st.caption('ì˜ì–´ ver')
        st.markdown(lyrics_eng_list[8])
    st.markdown('')
with tab5:
    st.subheader('TOP 5')
    st.markdown('**ì œëª©** : {0}'.format(top_five_df_1.iloc[4]['name']))
    st.markdown('**ê°€ìˆ˜** : {0} '.format(top_five_df_1.iloc[4]['Artist']))
    st.markdown('**url** : {0} '.format(top_five_df_1.iloc[4]['url']))
    st.markdown('**ìœ ì‚¬ë„** : {0:.4f}'.format(top_five_df_1.iloc[4]['book']))
    with st.expander('ê°€ì‚¬'):
        st.caption('ì›ë³¸ ver')
        st.markdown(lyrics_list[9])
        st.caption('ì˜ì–´ ver')
        st.markdown(lyrics_eng_list[9])
